{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: From messy to tidy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas library for Python was build around the dataframe idea taken from R, the statistical programming language. Wes McKinney is the driving force behind the library (O'Reilly book: Python for Data Analysis).\n",
    "\n",
    "Hadley Wickham is his R counterpart working on RStudio, the free programming environment for R, and author of some important R libraries.\n",
    "\n",
    "Hardly any flame wars between the R and Python communities. McKinney and Wickham sometimes work together closely, the fruits of which find their way into both languages. R is real strong in hard core statistical libraries and has a kind of functional twist to it and, at least for me, a bit of a quirky syntax; Python is the more broad programming language with strong support, through its libraries, for scientific programming.\n",
    "\n",
    "Both languages have \"notebooks\", and it is possible in the Jupyter ([JU]lia[PYT]hone[R]) noteboooks to incorporate both Python and R snippets. CSV files are the \"lingua franca\" between the languages.\n",
    "\n",
    "In 2014 Hadley Wickham wrote an important article in the Journal of Statistical Software: \"Tidy Data\".\n",
    "\n",
    "In it he argued for a certain way of structuring data in order to make it more easy and effective to clean and work with the data: Using consistent data structures and matching tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tidy structure has the following attributes:\n",
    "\n",
    "  - Each variable forms a column and contains values\n",
    "  - Each observation forms a row\n",
    "  - Each observational unit forms a table\n",
    "  \n",
    "  where:\n",
    "  \n",
    "  - variable is a measurement or an attribute (height, weight, sex, etc.)\n",
    "  - value is the actual measurement or attribute (152 cm, 80 kg, female, etc.)\n",
    "  - observation: all values measure on the same unit\n",
    "  \n",
    "A dataset that is not tidy is messy.\n",
    "\n",
    "Why are there messy datasets? Well, life is messy in a way. Often datasets get messy because they are used for presentation purposes and values of variables tend to creep into column headers. Or, in order to facilitate the input of data, one stores multiple variables into one column. Or, someone is being creative and bends the rules a bit in order to get things done (the restaurant dataset where rootbeer was accounted for as \"cola w/cheese\", simply because rootbeer was not available as a choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get some working experience with Pandas we will start to struggle a bit with messy datasets and tidy them up. Later on we will  focus more on analyzing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a firmer grasp on the problem, let's look a a very simple, but slightly messy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we open the file in an editor to have a look at it. A quick repair is to name the missing header.\n",
    "\n",
    "Then we use Pandas to read in the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/treatment.csv\", sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column containing name values is not named (has no header); the other two column headers contain values. The 5 or 6 values (depending on how we count the \"-\") in the cells are not given a proper variable name (header), they are just framed by the other values. This lay-out is perfectly ok for presentation purposes, but in order to process the data, we need a clear cut difference between variables and values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added the column name for the first column and saved the file as \"treatment1.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/treatment1.csv\", sep=\";\")\n",
    "\n",
    "melted_df = pd.melt(df,\n",
    "                   [\"Name\"],\n",
    "                   var_name = \"Treatment\",\n",
    "                   value_name = \"Result\")\n",
    "melted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column headers are values, not variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "\n",
    "df = pd.read_csv(\"./data/pew-raw.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = pd.melt(df,\n",
    "                      [\"religion\"],\n",
    "                      var_name = \"income\",\n",
    "                      value_name = \"freq\")\n",
    "formatted_df = formatted_df.sort_values(by = [\"religion\"])\n",
    "formatted_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = pd.read_csv(\"./data/billboard.csv\", encoding = \"mac_latin2\")\n",
    "df_songs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file above has two big drawbacks: Again values in the column headers (x1st.week, etc.) and when a song is in the Top 100 for less then 75 weeks, the remaining columns are filled with missing values (NaN).\n",
    "\n",
    "Now that we know the problems, let's make a plan to fix them:\n",
    "\n",
    "- we will store the week numbers as values in a single column (melt them into a date column)\n",
    "- we will create one row per week for each record (if there is no data for the given week, we will NOT create a row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the first 7 columns of the dataframe are ok\n",
    "# We will store their names in a list\n",
    "id_vars = [\"year\",\n",
    "          \"artist.inverted\",\n",
    "          \"track\",\n",
    "          \"time\",\n",
    "          \"genre\",\n",
    "          \"date.entered\",\n",
    "          \"date.peaked\"]\n",
    "\n",
    "# Now we can start to melt the weeks into a week variable and the ranking number into rank value\n",
    "# All the heavylifting is done by the melt fuction of Pandas\n",
    "df = pd.melt(frame=df_songs,\n",
    "            id_vars = id_vars,\n",
    "            var_name = \"week\",\n",
    "            value_name = \"rank\")\n",
    "# Quick look to see what we did\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values in the week column can be polished a bit\n",
    "# We just need the number between \"x\"[Int]\"st.week\"\n",
    "# And while we are at it: We can do without the float in the rank column\n",
    "# Formatting to the rescue\n",
    "import re\n",
    "df['week'] = df['week'].str.extract('(\\d+)',expand = False).astype(int)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, bummer; a whopping error. We forgot that our rank column, after the melting, contains all these NaN values and Python complained that it did not know how to convert \"NaN\" into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check\n",
    "print(df['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Away with the thing!\" We use the dropna() function on our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df['rank'] = df['rank'].astype(int)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add values for the new date column\n",
    "We have date.entered values and we have an integer for week\n",
    "With these two values we can compute the values for our new date column\n",
    "With the help of Pandas using the Python datetime library, we:\n",
    "- convert date.entered\n",
    "- convert the week value\n",
    "- add the two up\n",
    "- subtract the offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "pd.to_datetime(\"2009-09-23\")\n",
    "#pd.to_timedelta(1, unit='w')\n",
    "#pd.to_datetime(\"2009-09-23\") + pd.to_timedelta(1, unit='w')\n",
    "#pd.DateOffset(weeks=1)\n",
    "#pd.to_datetime(\"2009-09-23\") + pd.to_timedelta(2, unit='w') - pd.DateOffset(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to populate the new date column, we just have to add the new column\n",
    "df['date'] = pd.to_datetime(df['date.entered']) + pd.to_timedelta(df['week'], unit='w') - pd.DateOffset(weeks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a better overview of the rise and fall of records in the chart, we need to sort the dataframe\n",
    "We construct a new dataframe using a nested list of lists; leaving out date.entered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"year\",\n",
    "        \"artist.inverted\",\n",
    "        \"track\",\n",
    "        \"time\",\n",
    "        \"genre\",\n",
    "        \"week\",\n",
    "        \"rank\",\n",
    "        \"date\"]]\n",
    "df = df.sort_values(ascending = True, by = [\"year\", \"artist.inverted\", \"track\", \"week\", \"rank\"])\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come a long way, but our dataframe is still messy in the sense that in one dataframe or table we combine two observational units: song and rank. Two observational units should be presented in two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we store our dataframe in a new variable: billboard\n",
    "billboard = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then create a songs table that contains the details of each song\n",
    "# First we define the columns for that table:\n",
    "songs_cols = [\"year\",\n",
    "             \"artist.inverted\",\n",
    "             \"track\",\n",
    "             \"time\",\n",
    "             \"genre\"]\n",
    "songs = billboard[songs_cols].drop_duplicates()\n",
    "songs = songs.reset_index(drop = True)\n",
    "songs[\"song_id\"] = songs.index\n",
    "songs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a rank table that just contains the newly generated song_id together with date and rank\n",
    "ranks = pd.merge(billboard, songs, on = [\"year\", \"artist.inverted\", \"track\", \"time\", \"genre\"])\n",
    "ranks = ranks[[\"song_id\", \"date\", \"rank\"]]\n",
    "ranks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple variables stored in one column\n",
    "\n",
    "We use part of a dataset from the WHO that documents the count of confirmed tuberculosis cases by country, year, age and sex: tb-raw.csv.\n",
    "\n",
    "Let's have a quick look at the dataset to see what we have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/tb-raw.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately see some problems with this dataset:\n",
    "\n",
    "  - some columns contain multiple values (sex and age): m014 (male, 0-14)\n",
    "  - a mixture of zeros (0) and missing values (NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first round of tidying: We need to melt the sex + age group columns into a single one, after that we derive three columns from it: sex, age_lower, and age_upper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(df, id_vars=[\"country\",\"year\"], value_name=\"cases\", var_name=\"sex_and_age\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. Now we need to extract the three items of information from our new sex_and_age column. That is we want to store m014 separate columns for sex (i.e. \"m\"), age lower bound (\"0\") and age upper bound (\"14\").\n",
    "\n",
    "We do this by defining a temporary dataframe, that we fill with just the sex_and_age column. Then we will use a regular expression to split \"m014\" into the 3 items we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[\"sex_and_age\"].str.extract(\"(\\D)(\\d+)(\\d{2})\", expand=False)\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add names to the columns we just created\n",
    "tmp_df.columns = [\"sex\", \"age_lower\", \"age_upper\"]\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could leave it like this, but to keep a little bit closer to the original dataset, we are going to merge the \"age_lower\" and \"age_upper\" columns into one \"age\" column (still in our temporary dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df[\"age\"] = tmp_df[\"age_lower\"] + \"-\" + tmp_df[\"age_upper\"]\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, nearly there. We merge the two dataframes into one with the pandas \"concat\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, tmp_df], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now drop unnecessary columns and rows\n",
    "# We do not need sex_and_age, age_lower and age_upper any more, so we simply \"drop\" them\n",
    "df = df.drop(['sex_and_age', 'age_lower', 'age_upper'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last bits of tidying\n",
    "# We remove the missing values\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we sort the remaining rows\n",
    "df = df.sort(ascending = True, columns = ['country', 'year', 'sex', 'age'])\n",
    "# df = df.sort_values(by = ['country', 'year', 'sex', 'age'], ascending = True)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we scratched the surface here of what you can do with Pandas. We concentrated on tidying data, because that is what you will have almost always to do in order to get anywhere. Luckily, Pandas comes with excellent [documentation](http://pandas.pydata.org/pandas-docs/stable/).\n",
    "\n",
    "We think the combination of Pandas with Jupyter noteboooks is a powerful one. Especially, if we look at the possibilities to visualize the data contained in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "df.boxplot(column = 'cases', by = 'country')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
