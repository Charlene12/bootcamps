{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLI: Ch. 4 Creating Reusable Command-Line Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we crafted the following one liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5577 and\r\n",
      "   4187 the\r\n",
      "   2592 a\r\n",
      "   2510 to\r\n",
      "   2416 i\r\n",
      "   1969 it\r\n",
      "   1560 t\r\n",
      "   1531 was\r\n",
      "   1518 of\r\n",
      "   1334 he\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "! curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt | gunzip -c | tr '[:upper:]' '[:lower:]' | grep -oE '\\w+' | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something we might want to use every now and then. We can do several things: Turn it into a bash file, that we can run from the commandline (easy enough, we just have to line up the steps we worked out in a text file:\n",
    "\n",
    "    curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt | # download the zipped ebook\n",
    "    gunzip -c |                                               # unzip the zipped file\n",
    "    tr '[:upper:]' '[:lower:]' |                              # convert the entire text to lowercase\n",
    "    grep -oE '\\w+' |                                          # extract all words and place them on sep.\n",
    "    sort |                                                    # sort them in alphabetical order\n",
    "    uniq -c |                                                 # remove duplicates, count frequency of words\n",
    "    sort -nr |                                                # sort by count in reverse order\n",
    "    head -n 20                                                # show first 20 results\n",
    "\n",
    "Then we stuck the she-bang on top: #!/usr/bin/env BASH and change the file permissions to make it executable with: chmod u+x [file]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we were to use this script more often, some things will prove to be a bit cumbersome:\n",
    "\n",
    "    - if we want to process a new file, we have to open the file and put in the new URL of that file\n",
    "    - we might want to show a different number of results\n",
    "    \n",
    "These are small changes to the file, using variables that we can pass in at the commandline when calling the pipeline:\n",
    "\n",
    "    #!/usr/bin/env bash\n",
    "    NUM_WORDS=\"$1\"\n",
    "    ...\n",
    "    uniq -c | sort -nr | head -n $NUM_WORDS\n",
    "    \n",
    "Larger changes will become necessary when you do a lot of text analysis. When you run your script on another book, you will probably find that \"and\", \"the\", \"a\", and \"to\" are the most used words in that text too. We can filter these so-called stopwords out of course, often a file aptly called \"stopwords\" is used for that purpose.\n",
    "\n",
    "Then, you will probably formulate another question: \"What are the most used words in a text that are most typical for that text\"? Or any question you might have. NB. the question itself is not relevant here, but the fact that your programs will keep evolving over time:\n",
    "\n",
    "    - you will refactor stuff, abstract things out, move code to separate modules, etc.\n",
    "    - you will define new questions you want to answer, making changes in the code necessary\n",
    "    - you will other ideas that make you pull in other kinds of data that needs to be processed in other ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot with the commandline, especially in the context of (data) analysis: What is there?, How does it look like?, What is missing here?, etc., etc. But at a certain point there is a trade-off between ease of use and quickness and ease of re-use and thoroughness. At that point porting your solutions to another context, like a programming language like Python or R, might be a good road to follow. Especially because, as we will see later on, these languages come with libraries (batteries included) that went through the process we sketched above (refactoring, abstracting out, etc.) preparing these modules to work for others too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is our shell script in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "num_words = int(sys.argv[1])\n",
    "text = sys.stdin.read().lower()\n",
    "words = re.split('\\W+', text)\n",
    "cnt = Counter(words)\n",
    "for word, count in cnt.most_common(num_words):\n",
    "    print(\"%7d %s\" % (count, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And suppose you are really into text analysis, then Python offers the NLTK libraries to work with or PyTextrank. If your interest lies with data analysis then Python offers Pandas. We will dive into these topics later on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
